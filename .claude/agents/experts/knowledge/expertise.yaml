# Knowledge Management Expertise
# Target: 850-950 lines | Domain: Operational knowledge for book content management
# Size: 966 lines (WARNING THRESHOLD - pruning executed, target 750 next cycle)

overview:
  description: |
    Knowledge management and content capture—entry location decision tree,
    content structure patterns, voice guidelines, linking strategies, and
    question-driven development. This expertise enables correct placement,
    organization, and development of book content.
  scope: |
    Covers book directory structure (chapters/, appendices/), content frontmatter,
    entry type decisions (new vs extend), inline timestamp patterns, voice guidelines,
    cross-reference strategies, and question file management. Does NOT cover agent
    authoring (see agent-authoring expert) or book structure mechanics like TOC
    generation (see book-structure expert).
  rationale: |
    Effective knowledge capture enables insights to be discoverable, properly
    contextualized, and integrated with related content. Poor knowledge management
    leads to fragmented insights, lost context, and duplication.

core_implementation:
  primary_files:
    - path: chapters/
      purpose: All book content organized by parts and chapters
    - path: appendices/examples/
      purpose: Real project configurations and examples
    - path: .journal/
      purpose: Timestamped personal thoughts (hidden, private)
    - path: CLAUDE.md
      lines: 34-117
      purpose: Book structure conventions and content conventions
    - path: STYLE_GUIDE.md
      purpose: Voice, evidence standards, structural requirements

  key_patterns:
    - name: Entry Location Decision
      summary: Where new content belongs based on topic area
    - name: Inline Timestamp Pattern
      summary: Adding dated insights to existing content
    - name: Question-Driven Development
      summary: Using _questions.md to guide chapter expansion
    - name: Cross-Reference Pattern
      summary: Bidirectional linking between related content

key_operations:
  determine_entry_location:
    name: Determine Where Content Belongs
    description: Decide location for new insights in the book structure
    when_to_use: Capturing new insights, adding content, expanding topics
    approach: |
      Use decision_trees > entry_location_framework for full mapping.
      Quick reference: Part 1 (foundations), Part 2 (patterns/practices),
      Part 3 (mental models/tools), Part 4 (examples/appendices).
      Then: extend existing vs create new (see new_vs_extend_decision tree).

  choose_new_vs_extend:
    name: Create New Entry vs Extend Existing
    description: Decide whether to create new file or extend existing content
    when_to_use: Adding new insights that relate to existing content
    approach: |
      Extend existing entry when:
      - Insight directly relates to existing content scope
      - Adds nuance or example to established pattern
      - Answers question posed in entry
      - Fills gap in existing coverage

      Create new entry when:
      - Insight doesn't fit existing entry's scope
      - Topic warrants dedicated exploration
      - Content is significantly different from existing
      - Likely to grow independently
    examples:
      - scenario: New prompt structuring technique
        decision: Extend chapters/2-prompt/2-structuring.md
        reasoning: Fits existing scope, adds to pattern catalog
      - scenario: New mental model about agent loops
        decision: Create chapters/8-mental-models/N-new-model.md
        reasoning: Distinct concept, warrants dedicated exploration

  apply_voice_guidelines:
    name: Apply Voice and Tone Guidelines
    description: Ensure content matches established voice patterns
    when_to_use: Writing or reviewing any book content
    approach: |
      See best_practices > Voice and Tone for comprehensive guidance.
      Key principles: third-person throughout, avoid hedging, imperative mood
      for instructions, bold assertion + elaboration for mental models.

  use_inline_timestamps:
    name: Add Timestamped Inline Additions
    description: Extend existing content with dated insights
    when_to_use: Adding new learning to existing section without restructuring
    approach: |
      Format: *[YYYY-MM-DD]*: Insight text here...

      When to use:
      - Adding new learning to existing section
      - Documenting experiential observation
      - Noting pattern discovered in implementation
      - Recording evolution of understanding

      Placement:
      - Add after related content in section
      - Group related timestamp entries together
      - Maintain chronological order within groups
    examples:
      - scenario: New insight about context management
        format: |
          *[2025-12-08]*: Multi-agent systems benefit from explicit context handoff
          protocols. Without clear boundaries, context can drift between agents.
      - scenario: Pattern from implementation
        format: |
          *[2025-12-09]*: Decision framework pattern from model-selection.md -
          ASCII flowcharts communicate decision processes more clearly than prose.

  manage_question_development:
    name: Use Questions to Drive Content Development
    description: Leverage _questions.md files to guide chapter expansion
    when_to_use: Planning content development, expanding chapters
    approach: |
      See patterns > question_file_pattern for comprehensive guidance.
      Key principle: Answers go in chapter content, NOT in _questions.md.
      _questions.md tracks state only using markers: (unmarked), [partial],
      [answered], [stale], [deferred].

  implement_cross_references:
    name: Create Effective Cross-References
    description: Link related content with contextual explanations
    when_to_use: Connecting related concepts across chapters
    approach: |
      Link patterns:
      - Use relative paths from current file
      - Include section anchors when specific: path.md#section
      - Add contextual explanation (why connection matters)

      Bidirectional linking:
      - When linking from A to B, also add link from B to A
      - Both links should explain the relationship

      Enhanced cross-reference format:
      Before (minimal):
      - **To [Tool Use](path.md):** Tool descriptions are prompts

      After (contextual):
      - **To [Tool Use](path.md):** Tool descriptions are prompts themselves.
        Poor tool docs lead to misuse regardless of main prompt quality.
    pitfalls:
      - what: Generic "click here" link text
        why: No context, poor for scanning
        instead: Descriptive text explaining the connection
      - what: One-directional links only
        why: Misses discovery opportunity from other side
        instead: Add bidirectional links

decision_trees:
  entry_location_framework:
    name: Entry Location Decision Tree
    entry_point: What is the content about?
    branches:
      - condition: Core concepts (prompts, models, context, tools)
        action: Part 1 - Foundations (chapters 1-5)
        sub_branches:
          - condition: Four pillars overview, leverage points
            action: chapters/1-foundations/
          - condition: Prompt structure/types/patterns
            action: chapters/2-prompt/
          - condition: Model selection/behavior/limitations
            action: chapters/3-model/
          - condition: Context management/loading/strategies
            action: chapters/4-context/
          - condition: Tool design/selection/restrictions
            action: chapters/5-tool-use/
      - condition: Patterns and practices
        action: Part 2 - Craft (chapters 6-7)
        sub_branches:
          - condition: Recurring architectural pattern
            action: chapters/6-patterns/
          - condition: Operational practice (debugging, eval, production)
            action: chapters/7-practices/
      - condition: Mental models and tooling
        action: Part 3 - Perspectives (chapters 8-9)
        sub_branches:
          - condition: Thinking framework or design principle
            action: chapters/8-mental-models/
          - condition: Specific tool documentation
            action: chapters/9-practitioner-toolkit/
      - condition: Examples and implementations
        action: Part 4 - Appendices (appendices/examples/)

  new_vs_extend_decision:
    name: Create New Entry vs Extend Existing
    entry_point: Does related content exist?
    branches:
      - condition: No existing coverage of topic
        action: Create new entry with appropriate frontmatter
      - condition: Existing entry covers related ground
        sub_branches:
          - condition: New content fits existing scope
            action: Extend existing entry with new section
          - condition: New content warrants dedicated exploration
            action: Create new entry, link from existing
          - condition: New content is a nuance or example
            action: Add inline with timestamp

  entry_scope_decision:
    name: Entry Scope Planning
    entry_point: How comprehensive should this entry be?
    branches:
      - condition: Chapter introduction (_index.md)
        action: Brief overview with links to sections (50-100 lines)
      - condition: Concept introduction
        action: Initial framing with leading questions (100-200 lines)
      - condition: Comprehensive reference
        action: Full pattern catalog with examples (400-650 lines)
      - condition: Mental model
        action: Focused framework with metaphors (150-250 lines)

patterns:
  content_structure_pattern:
    name: Standard Developed Entry Structure
    context: Fully developed chapter sections
    implementation: |
      Structure flow: questions → mental model → patterns

      1. Core Questions (categorized by theme)
         - Surface what the entry addresses
         - Provide navigation aid

      2. Your Mental Model
         - Bold assertion with practical elaboration
         - Frame conceptual understanding

      3. Domain Content
         - Patterns, implementations, examples
         - Actionable details

      4. Connections
         - Links to related entries with context
    trade_offs:
      - advantage: Multiple entry points for different reader needs
        cost: More structure to maintain
      - advantage: Questions explicitly acknowledge uncertainty
        cost: Requires thinking about what's unknown
    real_examples:
      - location: chapters/2-prompt/2-structuring.md
        note: Comprehensive entry with Core Questions, Mental Model, patterns

  inline_addition_pattern:
    name: Inline Timestamped Addition
    context: Adding insights to existing content without restructuring
    implementation: |
      Format: *[YYYY-MM-DD]*: New insight...

      Use cases:
      - Adding new learning to existing section
      - Documenting experiential observation
      - Noting pattern discovered in implementation

      Example:
      ## Context Management

      Effective context management requires careful prioritization.

      *[2025-12-08]*: Multi-agent systems benefit from explicit context
      handoff protocols. Without clear boundaries, context can drift.
    trade_offs:
      - advantage: Tracks when learning was captured
        cost: Visual noise in content
      - advantage: Preserves existing structure
        cost: May fragment related insights

  question_file_pattern:
    name: Question File Development Pattern
    context: Using _questions.md to drive content development
    implementation: |
      Purpose: Generative scaffolding, NOT book output

      Structure:
      - Questions grouped by theme/subtopic
      - State markers track progress
      - References to content that answers questions

      States:
      - (unmarked) - Fresh, unanswered
      - [partial] - Started, needs expansion
      - [answered] - Comprehensive coverage exists
      - [stale] - May need revisiting
      - [deferred] - Skipped intentionally

      Key principle:
      - Answers go in chapter content files
      - _questions.md tracks state only
    real_examples:
      - location: chapters/2-prompt/_questions.md
        note: Questions organized by theme with state tracking

  leading_questions_pattern:
    name: Leading Questions for New Entries
    context: Seeding new entries with questions for future development
    implementation: |
      ## Leading Questions

      - How does this pattern scale across different model sizes?
      - What are the failure modes when context exceeds limits?
      - How do you measure effectiveness of this approach?

      Purpose:
      - Guide future development
      - Explicitly acknowledge uncertainty
      - Create roadmap for expansion
    trade_offs:
      - advantage: Makes entry developable by others
        cost: Entry feels incomplete until answered

  contextual_cross_reference_pattern:
    name: Contextual Cross-References (Evolved Pattern)
    context: Linking related content with explanation. Enhanced via commit d69ef22 learnings.
    implementation: |
      ## Connections section format:
      - **To [Topic](relative/path.md)**: Why this connection matters.
        Additional context about the relationship.

      Enhanced contextual format:
      - **To [Tool Use](../5-tool-use/_index.md):** Tool descriptions are prompts
        themselves. Poor tool docs lead to misuse regardless of main prompt quality.

      For changelog integrations (NEW):
      - Use comparison tables to show feature trade-offs (not just describing separately)
      - Link to mental models through pattern explanations, not just tool features
      - Group related features under thematic headers before creating separate sections
      - Example: Real-Time Message Steering links conceptually to Progressive Refinement

      Add 1-2 sentences per link explaining:
      - Why the connection matters (conceptual bridge)
      - What insight readers should transfer between sections
    real_examples:
      - location: chapters/2-prompt/_index.md
        note: Basic Connections section
      - location: chapters/9-practitioner-toolkit/1-claude-code.md lines 337-366
        note: Unified Mental Model with comparison table (Skills vs Slash Commands)
      - location: chapters/9-practitioner-toolkit/1-claude-code.md lines 651-698
        note: Hook Context Injection with Block vs Context Injection decision table

best_practices:
  - category: Content Placement
    timestamp: 2025-12-26
    practices:
      - practice: Check existing content before creating new entries (CLAUDE.md:169)
      - practice: Prefer extending existing files over creating new ones (CLAUDE.md:170)
      - practice: Use entry location decision tree for placement

  - category: Voice and Tone
    timestamp: 2025-12-26
    practices:
      - practice: Use direct statements over hedging (STYLE_GUIDE.md)
      - practice: Third-person only throughout (STYLE_GUIDE.md:17-24, commit 26f7974)
      - practice: Bold assertion + elaboration for mental models (pit-of-success.md)
      - practice: Imperative mood for instructions (STYLE_GUIDE.md:37-44)

  - category: Content Structure
    timestamp: 2025-12-26
    practices:
      - practice: Lead with Core Questions for developed entries (commit 0322625)
      - practice: Use three-tier structure (questions → mental model → patterns)
      - practice: Tables for comparing 3-7 options across dimensions
      - practice: Lead complex patterns with concrete examples before abstraction
      - practice: Use timestamped inline additions for variant patterns

  - category: Evidence-Grounded Content (New Learnings)
    practices:
      - practice: Comprehensive evidence sections with research citations
        evidence: chapters/2-prompt/3-language.md (commit a624250) - 635 lines with academic papers, official docs, practitioner sources
        rationale: Long-form evidence-based chapters establish credibility and provide actionable research-backed guidance
        timestamp: 2025-12-26
      - practice: Structure research findings with clear tables showing quantified impact
        evidence: 3-language.md uses tables for task-type dependency, specificity calibration, model-specific patterns
        rationale: Tables enable quick scanning of evidence and direct comparison of approaches
        timestamp: 2025-12-26
      - practice: Separate academic papers, official documentation, and practitioner sources in references
        evidence: 3-language.md references section (lines 576-623) categorizes source types
        rationale: Clarifies evidence strength and helps readers assess claim validity
        timestamp: 2025-12-26
      - practice: Include arxiv IDs and specific paper names for reproducibility
        evidence: 3-language.md includes arxiv IDs like "SatLM arxiv:2305.09656" and "DETAIL Framework arxiv:2512.02246"
        rationale: Enables readers to verify claims and dive deeper into research
        timestamp: 2025-12-26
      - practice: Use "Open Questions" section to acknowledge uncertainty
        evidence: 3-language.md ends with open research questions (lines 625-633)
        rationale: Models intellectual humility and guides future exploration
        timestamp: 2025-12-26

  - category: Cross-References & Question Management
    timestamp: 2025-12-26
    practices:
      - practice: Bidirectional linking with contextual explanation (commit a72bcf2)
      - practice: Update related index files when adding content
      - practice: Answers go in chapter content, not in _questions.md
      - practice: Update question states immediately after addressing

  - category: Constraint Framing (Moved from patterns_from_recent_commits consolidation)
    practices:
      - practice: Frame constraints as positive requirements, not prohibitions
        evidence: chapters/2-prompt/3-language.md lines 188-263 conversion table
        rationale: Negative constraints ("never do X") backfire at scale; positive framing reduces semantic activation of unwanted patterns
        timestamp: 2025-12-26
        example: "|Never use global state|Use dependency injection|"

known_issues:
  - issue: Voice consistency can drift across long entries
    workaround: Review against voice guidelines during editing
    status: open
    timestamp: 2025-12-26

  - issue: False technical claims can propagate before validation
    workaround: Verify technical assertions against actual implementation
    context: Commit 26f7974 removed ~600 lines of hook enforcement claims proven false
    status: resolved
    timestamp: 2025-12-26
    learning: Technical claims about code capabilities must be verified against actual implementation, not assumed from documentation

  - issue: Question states can drift out of sync with content
    workaround: Periodic review of question files against chapter content
    status: open
    timestamp: 2025-12-26

  - issue: Cross-references can become stale when content moves
    workaround: Search for broken links after restructuring
    status: open
    timestamp: 2025-12-26

  - issue: Inline timestamps create visual noise
    workaround: Consider consolidating into dedicated sections for mature entries
    status: accepted
    timestamp: 2025-12-26

  - issue: Research-heavy chapters require significant time investment
    workaround: Plan for 20+ hours when committing to comprehensive evidence-based content
    context: chapters/2-prompt/3-language.md took substantial research effort
    status: accepted
    timestamp: 2025-12-26


documenting_system_patterns:
  name: Documenting System-Level Patterns (Multi-Agent Architectures)
  context: Pattern from chapters/6-patterns/2-self-improving-experts.md expansion
  timestamp: 2025-12-26

  pattern_observed: |
    System-level patterns (multi-agent architectures) require 5-section structure:

    1. **Evolution Timeline Table** - commit history as narrative backbone
       (chronological: hash | date | change | impact, highlight CRITICAL pivots)

    2. **System-Level Benefits Comparison** - emergent properties vs individual
       (before/after table showing dimensions like consistency, boundaries, clarity)

    3. **Template Documentation** - reusable pattern with real examples
       (role breakdown, tool allocation, frontmatter from actual files with line numbers)

    4. **Structured Schema** - detailed knowledge format specification
       (sections with purpose/structure/examples, line count targets, mutability strategy)

    5. **Case Study** - concrete application with quantified metrics
       (8-step process, before/after comparison, ROI calculation)

  key_techniques:
    - Git commits as verifiable evidence (commit hashes link to actual diffs)
    - Every claim backed by file path + line numbers
    - Real quantified metrics from actual usage (not theoretical estimates)
    - Tables for quick scanning (evolution, benefits, templates, comparisons)
    - Incremental growth via timestamped sections (preserves base + shows evolution)

  when_to_use:
    - Multi-agent system architectures (3+ coordinated agents)
    - Evolution from individual pattern to system paradigm
    - Reusable templates needing comprehensive reference

  real_examples:
    - location: chapters/6-patterns/2-self-improving-experts.md (lines 643-1465)
      note: 822 lines with 15+ commits, 20+ file paths, 10+ metrics, 5 tables

comprehensive_research_chapter_pattern:
  name: Comprehensive Evidence-Based Chapter Pattern
  context: Pattern from chapters/2-prompt/3-language.md creation (commit a624250)
  timestamp: 2025-12-26

  pattern_observed: |
    Creating substantial research-backed chapters (400-650 lines) that synthesize
    academic papers, official documentation, and practitioner sources into
    authoritative technical reference material.

    Structure for evidence-based chapters:
    1. Opening: Clear scope and relevance statement (what's covered, why it matters)
    2. Multiple focused sections (5-7 major topics per chapter)
    3. Evidence integration: Inline citations with quantified claims
    4. Comparison tables showing approaches across dimensions
    5. Anti-patterns section demonstrating what NOT to do
    6. Model-specific guidance when relevant (Claude, GPT, Gemini)
    7. Comprehensive references section (categorized by source type)
    8. Open questions section acknowledging research gaps

  key_metrics:
    evidence_density: "~15-20 citations per 600 lines"
    structure_ratio: "70% patterns/guidance, 20% examples, 10% references"
    time_investment: "Plan for 20+ hours of research and synthesis"
    source_categories: "3 types - academic papers, official docs, practitioner sources"

  citation_patterns:
    academic_papers: |
      Include arxiv IDs and paper names for reproducibility:
      - "SatLM: Declarative Prompting - arxiv:2305.09656"
      - "DETAIL Framework: Prompt Specificity Impact - arxiv:2512.02246"
      - "When A Helpful Assistant Is Not Really Helpful - EMNLP 2024, arxiv:2311.10054"

    quantified_claims: |
      Specific numbers from research, not vague assertions:
      - "23% improvement with declarative phrasing on reasoning tasks"
      - "+0.47 accuracy on mathematical tasks with added specificity"
      - "20-80% time increase for minimal accuracy gain with CoT on reasoning models"
      - "Up to 40% accuracy variance based solely on delimiter choice"

    official_documentation: |
      Link directly to provider documentation:
      - "Anthropic Claude Best Practices: docs.anthropic.com/claude/docs/prompt-engineering"
      - "OpenAI GPT-4.1 Prompting Guide: cookbook.openai.com"
      - "Google Gemini Prompting Strategies: ai.google.dev/docs/prompting-strategies"

  table_usage_patterns: |
    Tables for comparative analysis and quick scanning:

    Task-Type Dependency Table:
    | Task Type | Preferred Mood | Example | Rationale |
    Shows how different task categories require different linguistic approaches

    Specificity Impact Table:
    | Task Category | Specificity Gain | Details |
    Quantifies evidence for when to add detail vs when to stay flexible

    Model-Specific Patterns:
    Separate subsections for Claude (XML tags), GPT (Markdown), Gemini (context-last)
    with concrete code examples for each

  anti_patterns_section: |
    Dedicated "Anti-Patterns" section showing what NOT to do:
    - Pattern name and example
    - Why it's problematic
    - Better alternative

    Example from 3-language.md:
    - Over-Hedging: "Perhaps it might be worth..." → "Implement error recovery"
    - Anthropomorphization: "I know you're very smart..." → Direct task specification
    - Format Ambiguity: "Nice readable format" → Explicit JSON schema

  when_to_use:
    - Foundational topics where evidence-grounded guidance provides long-term reference value
    - Areas with significant academic/industry research to synthesize
    - Topics where practitioners need verifiable, authoritative claims
    - Content that will be cited or referenced extensively

  trade_offs:
    - advantage: Establishes authoritative, verifiable guidance
      cost: Requires significant research investment (20+ hours)
    - advantage: Long-term reference value for practitioners
      cost: Higher maintenance burden as research evolves
    - advantage: Enables readers to verify claims and explore deeper
      cost: More complex to write and structure than experience-based content

  real_examples:
    - location: chapters/2-prompt/3-language.md
      lines: 635 total
      sections: 9 major sections (verb semantics, specificity, constraints, delimiters, role/persona, CoT, model-specific, anti-patterns, connections)
      citations: "9 academic papers, 3 official docs, 4 practitioner sources"
      tables: 6 comparison/analysis tables
      note: "Synthesized 75+ sources into coherent practitioner guidance"

  integration_with_existing_content: |
    Evidence-based chapters reference and update related content:
    - Added cross-reference in chapters/2-prompt/1-prompt-types.md
    - Updated chapters/2-prompt/2-structuring.md with link
    - Updated chapters/2-prompt/_index.md to include new section
    - Bidirectional linking to chapters/3-model/2-model-behavior.md

    This creates knowledge web where research-backed insights connect
    to practical implementation guidance.


cross_domain_observations_pattern:
  name: Cross-Domain Learning Registry
  context: Pattern from .claude/agents/experts/.shared/observations.yaml (Dec 27, 2025)
  timestamp: 2025-12-27

  pattern_observed: |
    Multi-domain expert systems benefit from shared learning registry that enables
    expertise propagation beyond individual domains. The observations.yaml file provides:

    1. **cross_domain_patterns** - Successfully validated patterns with applicability lists
    2. **failed_adoptions** - Patterns that failed with documented reasons/lessons
    3. **contribution_protocol** - How improve agents add learnings
    4. **inheritance_protocol** - How domains validate and adopt patterns

  contribution_decision_criteria: |
    After updating domain expertise.yaml, assess cross-domain applicability:

    Pattern is cross-domain if:
    - Solves problem 2+ domains face (e.g., tool selection, safety protocols)
    - Evidence from actual domain usage (quantified where possible)
    - Clear applicability list (which domains benefit?)

    Pattern is domain-specific if:
    - Unique to domain's technical context (e.g., book frontmatter schema)
    - Workflow specific to domain operations (e.g., TOC generation)

  structure_format: |
    cross_domain_patterns:
      - pattern: <kebab-case-name>
        source: <originating-domain>
        observation: |
          Multi-line description of pattern and context
        applicability:
          - <domain1>
          - <domain2>
        evidence: |
          Quantified evidence from actual usage
        timestamp: YYYY-MM-DD

    failed_adoptions:
      - pattern: <kebab-case-name>
        attempted_in: <domain>
        reason: |
          Why pattern adoption failed
        lesson: |
          Meta-learning about when pattern doesn't apply
        timestamp: YYYY-MM-DD

  validation_approach: |
    Decentralized governance - no formal approval required. Domains mark patterns
    as validated via cross_domain_inheritance in their expertise.yaml. Contradictory
    observations trigger human review.

  when_to_contribute: |
    Knowledge domain contributions focus on:
    - Content organization patterns (applies to any documentation domain)
    - Voice consistency techniques (applies to any writing)
    - Cross-reference strategies (applies to interconnected content)
    - Evidence-grounded writing approaches (applies to technical documentation)

    Avoid contributing book-specific patterns (frontmatter schema, chapter hierarchy).

  real_examples:
    - location: .claude/agents/experts/.shared/observations.yaml
      patterns_documented: 3 cross-domain (tool-selection-consistency, dual-file-ownership, safety-protocol-migration)
      failed_adoptions: 2 (single-message-parallelism, question-agent-bash-access)

recent_content_learnings:
  model_native_swarm_documentation_pattern:
    name: Model-Native Swarm vs SDK Orchestration Documentation Pattern
    context: Pattern from chapters/3-model/4-multi-model-architectures.md expansion (commit e2e10c8)
    timestamp: 2026-01-30
    pattern_observed: |
      When documenting fundamentally new coordination paradigms (model-native vs SDK-based):

      1. **Lead with Architecture Contrast:** Use ASCII diagram showing structural difference
         - SDK orchestration: external framework calls spawning subagents
         - Model-native swarm: model reasoning spawns subagents internally
         - Visual contrast anchors conceptual understanding

      2. **Evidence Section with Training Details:** Document how capability emerged
         - Training approach (PARL - Parallel-Agent Reinforcement Learning)
         - Metrics that drove behavior (Critical Steps formula)
         - Why this approach works (serial collapse problem)
         - This grounds "magic" claims in verifiable training methodology

      3. **Quantified Performance Claims:** Specific numbers, not vague assertions
         - "3-4.5× wall-clock speedup" vs "much faster"
         - "Up to 100 concurrent subagents" vs "many subagents"
         - "80% runtime reduction" vs "significant improvement"

      4. **Comparison Table for Trade-offs:** Dimensions across coordination approaches
         - Coordination Logic, Subagent Spawning, Parallelism Limit
         - Developer Control, Debugging, Infrastructure, Latency, Token Overhead
         - Each cell: concrete difference, not abstract benefit claims

      5. **When to Use Decision Framework:** Practical criteria for paradigm selection
         - Good fit scenarios with rationale
         - Poor fit scenarios with rationale
         - Economic threshold guidance (when speedup justifies model cost)

      6. **Update All Cross-References:** Bidirectional linking to related patterns
         - Orchestrator Pattern section: add SDK vs model-native distinction
         - Execution Topologies: update branch limits (10 SDK vs 100+ model-native)
         - Connections section: explain relationship to existing patterns

    evidence: |
      chapters/3-model/4-multi-model-architectures.md lines 277-420:
      - 158 new lines documenting Kimi K2.5 model-native swarm
      - Architecture diagram (lines 295-309)
      - PARL training evidence section (lines 321-380)
      - Quantified performance: 3-4.5× speedup, 100 subagents, 80% reduction
      - SDK vs Model-Native comparison table (lines 358-382, 11 dimensions)
      - When to Use criteria (lines 396-419)
      - Updated 4 Connections entries to reference swarm distinction

      Cross-reference updates:
      - chapters/6-patterns/3-orchestrator-pattern.md lines 58-66: SDK vs swarm section
      - chapters/8-mental-models/5-execution-topologies.md lines 100-101: branch limits update

    real_examples:
      - location: chapters/3-model/4-multi-model-architectures.md
        lines: "277-420"
        note: Complete model-native swarm section with evidence, comparisons, decision framework

  file_based_coordination_documentation_pattern:
    name: File-Based Multi-Agent Coordination Documentation Pattern
    context: Pattern from chapters/9-practitioner-toolkit/1-claude-code.md TeammateTool section
    timestamp: 2026-01-30
    pattern_observed: |
      When documenting hidden/gated coordination features:

      1. **Feature Status Transparency:** Document gating mechanism upfront
         - "Server-Side Gated" subsection explaining unlock requirements
         - Community bypass path (claude-sneakpeek) with rationale
         - Anthropic's signal interpretation (production-ready but controlled rollout)

      2. **Mental Model Before Mechanics:** Frame conceptual layer before operations
         - "Your Mental Model" section: TeammateTool is coordination primitive layer
         - Analogy: Task = fork(), TeammateTool = process orchestration framework
         - Helps readers understand abstraction level difference

      3. **Operations Catalog with File Evidence:** Document primitives with observable state
         - 13 coordination operations listed with purpose
         - File-based messaging explanation (~/.claude/teams/{name}/inboxes/)
         - Directory structure example showing actual paths
         - Enables verification and debugging via filesystem inspection

      4. **Five Coordination Patterns with ASCII Diagrams:** Standard patterns enabled
         - Leader-Worker, Swarm, Pipeline, Council, Plan Approval (HITL)
         - Each pattern: diagram, use case, flow description
         - Real-world analogy for each (ETL for Pipeline, multi-perspective for Council)

      5. **Comparison Table: Task vs TeammateTool:** Capability matrix
         - 8 capabilities compared across both tools
         - Checkmarks show support, rationale in description
         - Makes decision framework concrete ("If you need X, use Y")

      6. **When to Use Decision Framework:** Flow chart format
         - Binary questions leading to recommendations
         - Accounts for: official support, communication needs, approval gates, patterns
         - Actionable guidance replacing abstract trade-off discussion

    evidence: |
      chapters/9-practitioner-toolkit/1-claude-code.md lines 243-531:
      - 287 new lines documenting TeammateTool coordination layer
      - Mental Model section (lines 248-252)
      - 13 operations documented (lines 257-269)
      - File-based messaging with directory structure (lines 271-291)
      - 5 coordination patterns with ASCII diagrams (lines 297-416)
      - Task vs TeammateTool comparison table (lines 426-441, 8 capabilities)
      - When to Use decision framework (lines 443-472)
      - Cross-reference to Multi-Agent Context patterns (line 410)

    real_examples:
      - location: chapters/9-practitioner-toolkit/1-claude-code.md
        lines: "243-531"
        note: Complete TeammateTool section with patterns, comparisons, decision framework

  cross_reference_for_new_concepts_pattern:
    name: Cross-Reference Strategy for New Concepts
    context: Pattern from swarm orchestration cross-references (commits e2e10c8)
    timestamp: 2026-01-30
    pattern_observed: |
      When adding new concepts that relate to existing patterns:

      1. **Bidirectional Linking with Context Updates:** Both files explain relationship
         - From new content → existing patterns: "See X for related coordination approach"
         - From existing patterns → new content: "New paradigm Y offers alternative approach"
         - Each link includes 1-2 sentences explaining conceptual bridge

      2. **Update Connections Section in New Content:** Explicit relationship documentation
         - To [Orchestrator Pattern]: SDK coordination vs model-native distinction
         - To [Cost and Latency]: Token overhead comparison (coordination in reasoning vs API calls)
         - To [Context Management]: Context sharing (internal vs duplicated across calls)
         - To [Model Selection]: New selection criterion (model-native swarm capability?)
         - Each entry adds specific new insight from new concept's perspective

      3. **Update Existing Pattern Content (Not Just Connections):** Inline context additions
         - Orchestrator Pattern: New "SDK vs Model-Native Swarm" subsection
         - Execution Topologies: Update branch count limits (SDK: 10, model-native: 100+)
         - These updates integrate new concept into existing mental models, not just append links

      4. **Preserve Link Symmetry:** Verify bidirectional references exist
         - New content links to existing patterns ✓
         - Existing patterns updated to link back to new content ✓
         - Cross-reference chain remains navigable in both directions

    evidence: |
      Model-native swarm orchestration integration:
      - chapters/3-model/4-multi-model-architectures.md lines 736-746: 4 updated Connections
      - chapters/6-patterns/3-orchestrator-pattern.md lines 58-66: New SDK vs Model-Native section
      - chapters/8-mental-models/5-execution-topologies.md lines 100-101: Branch limits update

      TeammateTool integration:
      - chapters/9-practitioner-toolkit/1-claude-code.md line 410: Link to Multi-Agent Context
      - chapters/6-patterns/3-orchestrator-pattern.md line 417: Link back to TeammateTool

      Pattern: New concept documented comprehensively (150+ lines), then existing content
      updated to reference it (2-5 line additions per related pattern). Total ~160-180 lines
      across 4-6 files creates integrated knowledge web.

    real_examples:
      - location: Multiple files
        note: Swarm orchestration required 4 cross-reference updates + 2 inline context additions

recent_content_learnings:
  mental_model_entry_structure:
    name: Mental Model Entry Structure Pattern
    context: Pattern from chapters/8-mental-models/5-execution-topologies.md creation
    timestamp: 2026-01-17
    pattern_observed: |
      Mental model entries for technical frameworks require systematic structure:
      1. Opening definition linking concept to practitioner value
      2. Core Idea section with boundaries/scope visualization
      3. Taxonomy section with consistent subsection template
      4. Measurement/metrics section for each taxonomy element
      5. Book mapping (connect to existing chapter content)

      Each taxonomy element follows template:
      - Definition
      - ASCII diagram (visual representation)
      - Book Mapping: [Pattern Name](../path.md) with context
      - When to Use: Bulleted criteria
      - Measurement Indicators: Observable metrics

      This structure enables practitioners to: understand the concept, classify
      their situation, apply appropriate patterns, and measure effectiveness.
    evidence: |
      chapters/8-mental-models/5-execution-topologies.md (415 lines):
      - 5 topologies (parallel, sequential, synthesis, nested, persistent)
      - Each with consistent structure (definition, diagram, book mapping, when to use, metrics)
      - Book mappings reference existing chapters (6-patterns/, 4-context/)
      - Measurement indicators provide observable assessment criteria
    real_examples:
      - location: chapters/8-mental-models/5-execution-topologies.md
        note: Lines 63-415 demonstrate pattern across 5 topologies

  changelog_integration_pattern:
    name: External Changelog Integration to Book Content
    timestamp: 2026-01-17
    pattern_observed: |
      When external tool releases new features, sync to book content via:
      1. Research Phase: Analyze changelog delta
      2. Impact Mapping: Map features to affected chapters
      3. Integration: Add timestamped sections to relevant chapters
      4. Cross-Reference: Link between related feature updates
      5. Evidence: Cite changelog version numbers and official docs

      **Insertion Location Strategy (2026-01-25):**
      - Insert timestamped entries between related existing sections
      - Single feature → inline entry; 2-3 related → subsection; 150+ lines → new section

    evidence: |
      Commit d69ef22 synced Claude Code 2.1.5-2.1.9: 610 lines across 5 chapters.
      chapters/9-practitioner-toolkit/1-claude-code.md expanded 437 → 706 lines (61% growth)

  operational_threshold_documentation:
    name: Operational Threshold Documentation Pattern
    timestamp: 2026-01-17
    pattern_observed: |
      When documenting features with operational thresholds:
      1. Threshold Table: Range | Signal | Recommended Action
      2. Numeric guidance with clear breakpoints (not vague "high/low")
      3. Decision framework based on thresholds

    evidence: chapters/4-context/2-context-strategies.md lines 123-150

  changelog_integration_cross_reference_strategy:
    name: Thematic Cross-Reference Pattern
    timestamp: 2026-01-25
    pattern_observed: |
      Connect changelog features through conceptual patterns, not just mechanical links:
      1. Mental Model Connection: Link to foundational concepts
      2. Trade-off Framing: Highlight when features solve competing concerns
      3. Evolution Narrative: Show how versions build on earlier capabilities

    evidence: chapters/9-practitioner-toolkit/1-claude-code.md embeds decision tables and
      "when to use" context that naturally reference related capabilities.

  large_scale_content_removal_pattern:
    name: Large-Scale Content Removal Pattern
    context: Pattern from commit 6dd8246 (remove private examples for open-source release)
    timestamp: 2026-01-21
    pattern_observed: |
      When removing large amounts of content:
      1. Clear commit messages indicating scope and reason
      2. Update all index files reflecting removal
      3. Update cross-references to point to remaining examples
      4. Synchronize CLAUDE.md directory listings with filesystem

    evidence: |
      Commit 6dd8246 removed ~22,000 lines (138 files) with coordinated updates
      to appendices/_index.md and cross-references. No broken links after removal.

  changelog_subsection_hierarchy_pattern:
    name: Multi-Level Subsection Organization for Feature Families
    context: Pattern from Keyboard Customization section in commit d69ef22
    timestamp: 2026-01-25
    pattern_observed: |
      When changelog features span multiple releases and have rich configuration details,
      use nested subsections to organize complexity without creating separate top-level sections.

      **Decision Criteria for Subsection Creation:**
      - Single feature, <50 lines → inline timestamped entry
      - Related features, 50-130 lines → new ## Main Section with ### Subsections
      - Unrelated features, >150 lines each → separate ## sections (different concepts)

      **Subsection Structure Pattern:**
      1. Definition Subsection (What/How): Core concept, first time mentioned
      2. Configuration Subsection (Custom/Advanced): Implementation details
      3. Context Subsection (Use Cases/Patterns): When/why to use
      4. Interaction Subsection (Compatibility/Edge Cases): Integration concerns

    evidence: |
      chapters/9-practitioner-toolkit/1-claude-code.md lines 101-237:
      New Keyboard Customization section (137 lines) spanning 3 releases uses 7 subsections
      to organize comprehensive configuration guidance without overwhelming readers.

    real_examples:
      - location: chapters/9-practitioner-toolkit/1-claude-code.md
        lines: "101-237"
        note: 7-level subsection hierarchy for changelog feature family

stability:
  oscillation_detection:
    rule: |
      IF entry E1 at T1 contradicts E0 at T0
      AND entry E2 at T2 contradicts E1
      THEN oscillation detected
    resolution: |
      Preserve both with conflict marker
      Escalate to human
    detected_oscillations: []

  convergence_indicators:
    insight_rate_trend: "Active - new coordination paradigms, cross-reference strategies, evidence patterns"
    new_entries_this_cycle: 3  # model-native swarm, file-based coordination, cross-reference for new concepts
    contradiction_count: 0
    last_reviewed: 2026-01-30
    notes: |
      Knowledge domain continues active learning from swarm orchestration documentation:
      - Model-native swarm vs SDK orchestration documentation pattern
      - File-based multi-agent coordination documentation (TeammateTool)
      - Cross-reference strategy for integrating fundamentally new concepts
      - Pruning executed: 981 → 966 lines (consolidation + new entries)

      Size governance: Executed consolidation to address 981-line warning threshold.
      Consolidated: 4 patterns (operational thresholds, cross-reference strategy, changelog
      integration, subsection hierarchy) reduced ~80 lines while preserving core insights.
      Added: 3 new patterns (~100 lines total)
      Net result: 966 lines (still above 900, needs aggressive pruning next cycle to reach 750)

      Recent patterns focus on paradigm documentation:
      1. How to document fundamentally new coordination approaches (SDK vs model-native)
      2. Evidence-grounded feature documentation (training details, quantified claims)
      3. Cross-referencing new concepts into existing mental models (inline updates, not just links)

      Domain remains stable with clear growth trajectory. No contradictions detected.
      Patterns converging toward: evidence-grounded claims, quantified performance, architectural comparisons.
