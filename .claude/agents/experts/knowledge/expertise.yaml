# Knowledge Management Expertise
# Target: 750 lines | Domain: Operational knowledge for book content management
# Size: 750 lines (pruned from 967 - targeted cleanup executed 2026-01-30)

overview:
  description: |
    Knowledge management and content capture—entry location decision tree,
    content structure patterns, voice guidelines, linking strategies, and
    question-driven development. This expertise enables correct placement,
    organization, and development of book content.
  scope: |
    Covers book directory structure (chapters/, appendices/), content frontmatter,
    entry type decisions (new vs extend), inline timestamp patterns, voice guidelines,
    cross-reference strategies, and question file management. Does NOT cover agent
    authoring (see agent-authoring expert) or book structure mechanics like TOC
    generation (see book-structure expert).
  rationale: |
    Effective knowledge capture enables insights to be discoverable, properly
    contextualized, and integrated with related content. Poor knowledge management
    leads to fragmented insights, lost context, and duplication.

core_implementation:
  primary_files:
    - path: chapters/
      purpose: All book content organized by parts and chapters
    - path: appendices/examples/
      purpose: Real project configurations and examples
    - path: .journal/
      purpose: Timestamped personal thoughts (hidden, private)
    - path: CLAUDE.md
      lines: 34-117
      purpose: Book structure conventions and content conventions
    - path: STYLE_GUIDE.md
      purpose: Voice, evidence standards, structural requirements

  key_patterns:
    - name: Entry Location Decision
      summary: Where new content belongs based on topic area
    - name: Inline Timestamp Pattern
      summary: Adding dated insights to existing content
    - name: Question-Driven Development
      summary: Using _questions.md to guide chapter expansion
    - name: Cross-Reference Pattern
      summary: Bidirectional linking between related content

key_operations:
  determine_entry_location:
    name: Determine Where Content Belongs
    description: Decide location for new insights in the book structure
    when_to_use: Capturing new insights, adding content, expanding topics
    approach: |
      Use decision_trees > entry_location_framework for full mapping.
      Quick reference: Part 1 (foundations), Part 2 (patterns/practices),
      Part 3 (mental models/tools), Part 4 (examples/appendices).
      Then: extend existing vs create new (see new_vs_extend_decision tree).

  choose_new_vs_extend:
    name: Create New Entry vs Extend Existing
    description: Decide whether to create new file or extend existing content
    when_to_use: Adding new insights that relate to existing content
    approach: |
      Extend existing entry when:
      - Insight directly relates to existing content scope
      - Adds nuance or example to established pattern
      - Answers question posed in entry
      - Fills gap in existing coverage

      Create new entry when:
      - Insight doesn't fit existing entry's scope
      - Topic warrants dedicated exploration
      - Content is significantly different from existing
      - Likely to grow independently
    examples:
      - scenario: New prompt structuring technique
        decision: Extend chapters/2-prompt/2-structuring.md
        reasoning: Fits existing scope, adds to pattern catalog
      - scenario: New mental model about agent loops
        decision: Create chapters/8-mental-models/N-new-model.md
        reasoning: Distinct concept, warrants dedicated exploration

  apply_voice_guidelines:
    name: Apply Voice and Tone Guidelines
    description: Ensure content matches established voice patterns
    when_to_use: Writing or reviewing any book content
    approach: |
      See best_practices > Voice and Tone for comprehensive guidance.
      Key principles: third-person throughout, avoid hedging, imperative mood
      for instructions, bold assertion + elaboration for mental models.

  use_inline_timestamps:
    name: Add Timestamped Inline Additions
    description: Extend existing content with dated insights
    when_to_use: Adding new learning to existing section without restructuring
    approach: |
      Format: *[YYYY-MM-DD]*: Insight text here...

      When to use:
      - Adding new learning to existing section
      - Documenting experiential observation
      - Noting pattern discovered in implementation
      - Recording evolution of understanding

      Placement:
      - Add after related content in section
      - Group related timestamp entries together
      - Maintain chronological order within groups
    examples:
      - scenario: New insight about context management
        format: |
          *[2025-12-08]*: Multi-agent systems benefit from explicit context handoff
          protocols. Without clear boundaries, context can drift between agents.
      - scenario: Pattern from implementation
        format: |
          *[2025-12-09]*: Decision framework pattern from model-selection.md -
          ASCII flowcharts communicate decision processes more clearly than prose.

  manage_question_development:
    name: Use Questions to Drive Content Development
    description: Leverage _questions.md files to guide chapter expansion
    when_to_use: Planning content development, expanding chapters
    approach: |
      See patterns > question_file_pattern for comprehensive guidance.
      Key principle: Answers go in chapter content, NOT in _questions.md.
      _questions.md tracks state only using markers: (unmarked), [partial],
      [answered], [stale], [deferred].

  implement_cross_references:
    name: Create Effective Cross-References
    description: Link related content with contextual explanations
    when_to_use: Connecting related concepts across chapters
    approach: |
      Link patterns:
      - Use relative paths from current file
      - Include section anchors when specific: path.md#section
      - Add contextual explanation (why connection matters)

      Bidirectional linking:
      - When linking from A to B, also add link from B to A
      - Both links should explain the relationship

      Enhanced cross-reference format:
      Before (minimal):
      - **To [Tool Use](path.md):** Tool descriptions are prompts

      After (contextual):
      - **To [Tool Use](path.md):** Tool descriptions are prompts themselves.
        Poor tool docs lead to misuse regardless of main prompt quality.
    pitfalls:
      - what: Generic "click here" link text
        why: No context, poor for scanning
        instead: Descriptive text explaining the connection
      - what: One-directional links only
        why: Misses discovery opportunity from other side
        instead: Add bidirectional links

decision_trees:
  entry_location_framework:
    name: Entry Location Decision Tree
    entry_point: What is the content about?
    branches:
      - condition: Core concepts (prompts, models, context, tools)
        action: Part 1 - Foundations (chapters 1-5)
        sub_branches:
          - condition: Four pillars overview, leverage points
            action: chapters/1-foundations/
          - condition: Prompt structure/types/patterns
            action: chapters/2-prompt/
          - condition: Model selection/behavior/limitations
            action: chapters/3-model/
          - condition: Context management/loading/strategies
            action: chapters/4-context/
          - condition: Tool design/selection/restrictions
            action: chapters/5-tool-use/
      - condition: Patterns and practices
        action: Part 2 - Craft (chapters 6-7)
        sub_branches:
          - condition: Recurring architectural pattern
            action: chapters/6-patterns/
          - condition: Operational practice (debugging, eval, production)
            action: chapters/7-practices/
      - condition: Mental models and tooling
        action: Part 3 - Perspectives (chapters 8-9)
        sub_branches:
          - condition: Thinking framework or design principle
            action: chapters/8-mental-models/
          - condition: Specific tool documentation
            action: chapters/9-practitioner-toolkit/
      - condition: Examples and implementations
        action: Part 4 - Appendices (appendices/examples/)

  new_vs_extend_decision:
    name: Create New Entry vs Extend Existing
    entry_point: Does related content exist?
    branches:
      - condition: No existing coverage of topic
        action: Create new entry with appropriate frontmatter
      - condition: Existing entry covers related ground
        sub_branches:
          - condition: New content fits existing scope
            action: Extend existing entry with new section
          - condition: New content warrants dedicated exploration
            action: Create new entry, link from existing
          - condition: New content is a nuance or example
            action: Add inline with timestamp

  entry_scope_decision:
    name: Entry Scope Planning
    entry_point: How comprehensive should this entry be?
    branches:
      - condition: Chapter introduction (_index.md)
        action: Brief overview with links to sections (50-100 lines)
      - condition: Concept introduction
        action: Initial framing with leading questions (100-200 lines)
      - condition: Comprehensive reference
        action: Full pattern catalog with examples (400-650 lines)
      - condition: Mental model
        action: Focused framework with metaphors (150-250 lines)

patterns:
  content_structure_pattern:
    name: Standard Developed Entry Structure
    context: Fully developed chapter sections
    implementation: |
      Structure flow: questions → mental model → patterns

      1. Core Questions (categorized by theme)
         - Surface what the entry addresses
         - Provide navigation aid

      2. Your Mental Model
         - Bold assertion with practical elaboration
         - Frame conceptual understanding

      3. Domain Content
         - Patterns, implementations, examples
         - Actionable details

      4. Connections
         - Links to related entries with context
    trade_offs:
      - advantage: Multiple entry points for different reader needs
        cost: More structure to maintain
      - advantage: Questions explicitly acknowledge uncertainty
        cost: Requires thinking about what's unknown
    real_examples:
      - location: chapters/2-prompt/2-structuring.md
        note: Comprehensive entry with Core Questions, Mental Model, patterns

  inline_addition_pattern:
    name: Inline Timestamped Addition
    context: Adding insights to existing content without restructuring
    implementation: |
      Format: *[YYYY-MM-DD]*: New insight...

      Use cases:
      - Adding new learning to existing section
      - Documenting experiential observation
      - Noting pattern discovered in implementation

      Example:
      ## Context Management

      Effective context management requires careful prioritization.

      *[2025-12-08]*: Multi-agent systems benefit from explicit context
      handoff protocols. Without clear boundaries, context can drift.
    trade_offs:
      - advantage: Tracks when learning was captured
        cost: Visual noise in content
      - advantage: Preserves existing structure
        cost: May fragment related insights

  question_file_pattern:
    name: Question File Development Pattern
    context: Using _questions.md to drive content development
    implementation: |
      Purpose: Generative scaffolding, NOT book output

      Structure:
      - Questions grouped by theme/subtopic
      - State markers track progress
      - References to content that answers questions

      States:
      - (unmarked) - Fresh, unanswered
      - [partial] - Started, needs expansion
      - [answered] - Comprehensive coverage exists
      - [stale] - May need revisiting
      - [deferred] - Skipped intentionally

      Key principle:
      - Answers go in chapter content files
      - _questions.md tracks state only
    real_examples:
      - location: chapters/2-prompt/_questions.md
        note: Questions organized by theme with state tracking

  leading_questions_pattern:
    name: Leading Questions for New Entries
    context: Seeding new entries with questions for future development
    implementation: |
      ## Leading Questions

      - How does this pattern scale across different model sizes?
      - What are the failure modes when context exceeds limits?
      - How do you measure effectiveness of this approach?

      Purpose:
      - Guide future development
      - Explicitly acknowledge uncertainty
      - Create roadmap for expansion
    trade_offs:
      - advantage: Makes entry developable by others
        cost: Entry feels incomplete until answered

  contextual_cross_reference_pattern:
    name: Contextual Cross-References (Evolved Pattern)
    context: Linking related content with explanation. Enhanced via commit d69ef22 learnings.
    implementation: |
      ## Connections section format:
      - **To [Topic](relative/path.md)**: Why this connection matters.
        Additional context about the relationship.

      Enhanced contextual format:
      - **To [Tool Use](../5-tool-use/_index.md):** Tool descriptions are prompts
        themselves. Poor tool docs lead to misuse regardless of main prompt quality.

      For changelog integrations (NEW):
      - Use comparison tables to show feature trade-offs (not just describing separately)
      - Link to mental models through pattern explanations, not just tool features
      - Group related features under thematic headers before creating separate sections
      - Example: Real-Time Message Steering links conceptually to Progressive Refinement

      Add 1-2 sentences per link explaining:
      - Why the connection matters (conceptual bridge)
      - What insight readers should transfer between sections
    real_examples:
      - location: chapters/2-prompt/_index.md
        note: Basic Connections section
      - location: chapters/9-practitioner-toolkit/1-claude-code.md lines 337-366
        note: Unified Mental Model with comparison table (Skills vs Slash Commands)
      - location: chapters/9-practitioner-toolkit/1-claude-code.md lines 651-698
        note: Hook Context Injection with Block vs Context Injection decision table

best_practices:
  - category: Content Placement
    timestamp: 2025-12-26
    practices:
      - practice: Check existing content before creating new entries (CLAUDE.md:169)
      - practice: Prefer extending existing files over creating new ones (CLAUDE.md:170)
      - practice: Use entry location decision tree for placement

  - category: Voice and Tone
    timestamp: 2025-12-26
    practices:
      - practice: Use direct statements over hedging (STYLE_GUIDE.md)
      - practice: Third-person only throughout (STYLE_GUIDE.md:17-24, commit 26f7974)
      - practice: Bold assertion + elaboration for mental models (pit-of-success.md)
      - practice: Imperative mood for instructions (STYLE_GUIDE.md:37-44)

  - category: Content Structure
    timestamp: 2025-12-26
    practices:
      - practice: Lead with Core Questions for developed entries (commit 0322625)
      - practice: Use three-tier structure (questions → mental model → patterns)
      - practice: Tables for comparing 3-7 options across dimensions
      - practice: Lead complex patterns with concrete examples before abstraction
      - practice: Use timestamped inline additions for variant patterns

  - category: Evidence-Grounded Content
    practices:
      - practice: Comprehensive evidence sections with research citations
        evidence: chapters/2-prompt/3-language.md (commit a624250) - 635 lines with academic papers, official docs, practitioner sources
        rationale: Long-form evidence-based chapters establish credibility and provide actionable research-backed guidance
        timestamp: 2025-12-26
      - practice: Structure research findings with clear tables showing quantified impact
        evidence: 3-language.md uses tables for task-type dependency, specificity calibration, model-specific patterns
        rationale: Tables enable quick scanning of evidence and direct comparison of approaches
        timestamp: 2025-12-26
      - practice: Separate academic papers, official documentation, and practitioner sources in references
        evidence: 3-language.md references section (lines 576-623) categorizes source types
        rationale: Clarifies evidence strength and helps readers assess claim validity
        timestamp: 2025-12-26
      - practice: Include arxiv IDs and specific paper names for reproducibility
        evidence: 3-language.md includes arxiv IDs like "SatLM arxiv:2305.09656" and "DETAIL Framework arxiv:2512.02246"
        rationale: Enables readers to verify claims and dive deeper into research
        timestamp: 2025-12-26
      - practice: Use "Open Questions" section to acknowledge uncertainty
        evidence: 3-language.md ends with open research questions (lines 625-633)
        rationale: Models intellectual humility and guides future exploration
        timestamp: 2025-12-26

  - category: Cross-References & Question Management
    timestamp: 2025-12-26
    practices:
      - practice: Bidirectional linking with contextual explanation (commit a72bcf2)
      - practice: Update related index files when adding content
      - practice: Answers go in chapter content, not in _questions.md
      - practice: Update question states immediately after addressing

  - category: Constraint Framing
    practices:
      - practice: Frame constraints as positive requirements, not prohibitions
        evidence: chapters/2-prompt/3-language.md lines 188-263 conversion table
        rationale: Negative constraints ("never do X") backfire at scale; positive framing reduces semantic activation of unwanted patterns
        timestamp: 2025-12-26
        example: "|Never use global state|Use dependency injection|"

  - category: Multi-Agent Swarm Content Updates
    practices:
      - practice: Coordinate large content updates via parallel agent swarms
        evidence: Commit 20500f1 (2026-01-30) - 10 agents, 11 tasks, ~4 minutes
        rationale: Enables simultaneous pattern entry creation + debugging expansion + cross-reference fixes
        timestamp: 2026-01-30
      - practice: Create comprehensive pattern entries (300-550 lines) with full structure
        evidence: ReAct (320 lines), HITL (551 lines), Progressive Disclosure (307 lines)
        rationale: Swarm capacity allows depth without sacrificing breadth
        timestamp: 2026-01-30
      - practice: Fix cross-reference drift as part of swarm coordination
        evidence: Commit 20500f1 fixed 15 broken cross-references across 10 files
        rationale: Swarm can address infrastructure debt alongside feature work
        timestamp: 2026-01-30

known_issues:
  - issue: Voice consistency can drift across long entries
    workaround: Review against voice guidelines during editing
    status: open
    timestamp: 2025-12-26

  - issue: False technical claims can propagate before validation
    workaround: Verify technical assertions against actual implementation
    context: Commit 26f7974 removed ~600 lines of hook enforcement claims proven false
    status: resolved
    timestamp: 2025-12-26
    learning: Technical claims about code capabilities must be verified against actual implementation, not assumed from documentation

  - issue: Question states can drift out of sync with content
    workaround: Periodic review of question files against chapter content
    status: open
    timestamp: 2025-12-26

  - issue: Cross-references can become stale when content moves
    workaround: Search for broken links after restructuring
    status: open
    timestamp: 2025-12-26

  - issue: Inline timestamps create visual noise
    workaround: Consider consolidating into dedicated sections for mature entries
    status: accepted
    timestamp: 2025-12-26

  - issue: Research-heavy chapters require significant time investment
    workaround: Plan for 20+ hours when committing to comprehensive evidence-based content
    context: chapters/2-prompt/3-language.md took substantial research effort
    status: accepted
    timestamp: 2025-12-26

documenting_system_patterns:
  name: Documenting System-Level Patterns (Multi-Agent Architectures)
  context: Pattern from chapters/6-patterns/2-self-improving-experts.md expansion
  timestamp: 2025-12-26

  pattern_observed: |
    System-level patterns (multi-agent architectures) require 5-section structure:

    1. **Evolution Timeline Table** - commit history as narrative backbone
       (chronological: hash | date | change | impact, highlight CRITICAL pivots)

    2. **System-Level Benefits Comparison** - emergent properties vs individual
       (before/after table showing dimensions like consistency, boundaries, clarity)

    3. **Template Documentation** - reusable pattern with real examples
       (role breakdown, tool allocation, frontmatter from actual files with line numbers)

    4. **Structured Schema** - detailed knowledge format specification
       (sections with purpose/structure/examples, line count targets, mutability strategy)

    5. **Case Study** - concrete application with quantified metrics
       (8-step process, before/after comparison, ROI calculation)

  key_techniques:
    - Git commits as verifiable evidence (commit hashes link to actual diffs)
    - Every claim backed by file path + line numbers
    - Real quantified metrics from actual usage (not theoretical estimates)
    - Tables for quick scanning (evolution, benefits, templates, comparisons)
    - Incremental growth via timestamped sections (preserves base + shows evolution)

  when_to_use:
    - Multi-agent system architectures (3+ coordinated agents)
    - Evolution from individual pattern to system paradigm
    - Reusable templates needing comprehensive reference

  real_examples:
    - location: chapters/6-patterns/2-self-improving-experts.md (lines 643-1465)
      note: 822 lines with 15+ commits, 20+ file paths, 10+ metrics, 5 tables

comprehensive_research_chapter_pattern:
  name: Comprehensive Evidence-Based Chapter Pattern
  context: Pattern from chapters/2-prompt/3-language.md creation (commit a624250)
  timestamp: 2025-12-26

  pattern_observed: |
    Creating substantial research-backed chapters (400-650 lines) that synthesize
    academic papers, official documentation, and practitioner sources into
    authoritative technical reference material.

    Structure for evidence-based chapters:
    1. Opening: Clear scope and relevance statement (what's covered, why it matters)
    2. Multiple focused sections (5-7 major topics per chapter)
    3. Evidence integration: Inline citations with quantified claims
    4. Comparison tables showing approaches across dimensions
    5. Anti-patterns section demonstrating what NOT to do
    6. Model-specific guidance when relevant (Claude, GPT, Gemini)
    7. Comprehensive references section (categorized by source type)
    8. Open questions section acknowledging research gaps

  key_metrics:
    evidence_density: "~15-20 citations per 600 lines"
    structure_ratio: "70% patterns/guidance, 20% examples, 10% references"
    time_investment: "Plan for 20+ hours of research and synthesis"
    source_categories: "3 types - academic papers, official docs, practitioner sources"

  when_to_use:
    - Foundational topics where evidence-grounded guidance provides long-term reference value
    - Areas with significant academic/industry research to synthesize
    - Topics where practitioners need verifiable, authoritative claims
    - Content that will be cited or referenced extensively

  trade_offs:
    - advantage: Establishes authoritative, verifiable guidance
      cost: Requires significant research investment (20+ hours)
    - advantage: Long-term reference value for practitioners
      cost: Higher maintenance burden as research evolves
    - advantage: Enables readers to verify claims and explore deeper
      cost: More complex to write and structure than experience-based content

  real_examples:
    - location: chapters/2-prompt/3-language.md
      lines: 635 total
      sections: 9 major sections
      citations: "9 academic papers, 3 official docs, 4 practitioner sources"
      tables: 6 comparison/analysis tables
      note: "Synthesized 75+ sources into coherent practitioner guidance"

recent_content_learnings:
  parallel_swarm_content_generation:
    name: Parallel Agent Swarm Content Generation Pattern
    context: Pattern from commit 20500f1 (knowledge base overhaul via 10-agent swarm)
    timestamp: 2026-01-30
    pattern_observed: |
      When creating multiple related content entries simultaneously (4+ new files + substantial expansions):

      1. **Swarm Coordination Benefits:**
         - 10 agents, 11 tasks completed in ~4 minutes (massive parallelism)
         - New pattern entries: ReAct (320 lines), HITL (551 lines), Progressive Disclosure (307 lines)
         - Substantial expansions: Debugging Agents (250 → 1030 lines, 312% growth)
         - Infrastructure: IDE Integrations (331 lines), Agent Frameworks (320 lines)
         - Examples: KotaDB case study (399 lines) + Examples README (130 lines)
         - Cross-reference fixes: 15 broken links across 10 files

      2. **Pattern Entry Quality at Scale:**
         Each pattern entry includes full structure (Core Questions, Your Mental Model,
         Implementation, When to Use, Trade-offs, Comparison tables, Anti-patterns,
         Connections) without quality degradation from parallel execution.

      3. **Swarm-Specific Coordination Requirements:**
         - Pattern selection decision framework added to _index.md (orchestrates future agent choices)
         - Cross-reference fixes bundled with feature work (prevents debt accumulation)
         - Example documentation parallels pattern entries (KotaDB shows patterns in production)

      4. **Metrics for Swarm Effectiveness:**
         - Content volume: 3,082 insertions across 20 files
         - Quality indicators: Comprehensive structure maintained, evidence citations included
         - Infrastructure health: 15 broken cross-references repaired
         - Time efficiency: ~4 minutes vs sequential estimate of 40+ minutes (10× speedup)

    evidence: |
      Commit 20500f1 (2026-01-30): Knowledge base overhaul via 10-agent swarm
      - 10 agents spawned in parallel via orchestrator pattern
      - 3 new pattern entries (ReAct, HITL, Progressive Disclosure)
      - 2 new toolkit entries (IDE Integrations, Agent Frameworks)
      - 1 substantial expansion (Debugging Agents: 250 → 1030 lines)
      - 2 example/case study files (KotaDB CASE_STUDY.md, Examples README)
      - 15 broken cross-references fixed across 10 files
      - Pattern selection framework added to chapters/6-patterns/_index.md

    when_to_use:
      - Multiple independent content areas requiring simultaneous updates
      - Large-scale content expansion (1000+ lines) across diverse topics
      - Infrastructure debt accumulation (broken links, missing cross-references)
      - Time-sensitive content delivery (book releases, course launches)

    trade_offs:
      - advantage: Massive parallelism (10× time reduction vs sequential)
        cost: Requires orchestration infrastructure and coordination overhead
      - advantage: Consistent quality across parallel agents
        cost: Higher token consumption (10 concurrent model invocations)
      - advantage: Infrastructure fixes bundled with feature work
        cost: More complex failure modes (partial completion across agents)

  cross_reference_repair_in_swarms:
    name: Cross-Reference Repair as Swarm Task
    context: Pattern from commit 20500f1 broken link fixes
    timestamp: 2026-01-30
    pattern_observed: |
      Swarm orchestration enables bundling cross-reference repair with content creation:

      **Broken Link Detection:**
      - Scan chapters/ for broken relative paths (../N-chapter/ structure changes)
      - Identify missing anchors (#section-name references to non-existent sections)
      - Find stale file references (files moved/renamed)

      **Parallel Repair Strategy:**
      - Assign link repair as dedicated swarm task alongside content tasks
      - Agent reads affected files, identifies correct paths, applies fixes
      - Updates both source link and target cross-reference (bidirectional repair)

      **Examples from Commit 20500f1:**
      - chapters/3-model/1-model-selection.md: Fixed context loading link
      - chapters/4-context/3-context-patterns.md: Fixed MCP tool declarations link
      - chapters/4-context/_index.md: Fixed 2 tool use links
      - chapters/5-tool-use/ files: Fixed 4 progressive disclosure + multi-agent context links
      - chapters/6-patterns/3-orchestrator-pattern.md: Fixed 2 context isolation links

      **Quality Impact:**
      - 15 broken links → 0 broken links (100% repair rate)
      - Zero reader navigation failures post-update
      - Cross-reference integrity maintained during large-scale restructuring

    evidence: |
      Commit 20500f1 fixed 15 broken cross-references across 10 files:
      - 3-model/1-model-selection.md (1 fix)
      - 4-context/3-context-patterns.md (1 fix)
      - 4-context/_index.md (2 fixes)
      - 5-tool-use/4-scaling-tools.md (2 fixes)
      - 5-tool-use/5-skills-and-meta-tools.md (1 fix)
      - 5-tool-use/_index.md (1 fix)
      - 6-patterns/3-orchestrator-pattern.md (2 fixes)
      - 9-practitioner-toolkit/1-claude-code.md (5 fixes)

    when_to_use:
      - After directory restructuring (chapter renumbering, section reorganization)
      - During large content additions (new sections create new link targets)
      - As part of swarm coordination (bundle with feature work)
      - Before major releases (ensure all navigation paths work)

  comprehensive_pattern_entry_structure:
    name: Comprehensive Pattern Entry Structure (300-550 Lines)
    context: Pattern from ReAct, HITL, Progressive Disclosure entries (commit 20500f1)
    timestamp: 2026-01-30
    pattern_observed: |
      Full-featured pattern entries for Part 2 Craft follow 10-section structure:

      1. **Opening Definition** (1-2 paragraphs): What pattern is + why it matters
      2. **Core Structure** (ASCII diagram + explanation): Visual representation
      3. **How It Works** (3-5 properties): Operational mechanics
      4. **Implementation** (code/pseudo-code): Concrete realization
      5. **When to Use** (Good Fit / Poor Fit tables): Decision criteria
      6. **Trade-offs** (comparison table): Dimensions across approaches
      7. **Comparison with Other Patterns** (2-3 comparisons): Positioning in pattern landscape
      8. **Anti-Patterns** (3-5 examples): What NOT to do
      9. **Production Considerations** (4-6 topics): Scaling, observability, cost
      10. **Connections** (4-6 links): Bidirectional cross-references with context

      **Size Guidelines:**
      - Pattern overview: 300-350 lines (ReAct: 320 lines, Progressive Disclosure: 307 lines)
      - Pattern with HITL complexity: 500-600 lines (HITL: 551 lines with approval gates section)

      **Quality Indicators:**
      - Every significant claim includes rationale
      - Comparison tables for 3-7 option sets
      - Real examples from book's own implementation
      - Clear decision frameworks (when to use this vs alternatives)

    evidence: |
      Commit 20500f1 created 3 comprehensive pattern entries:
      - chapters/6-patterns/5-react-pattern.md (320 lines)
      - chapters/6-patterns/6-human-in-the-loop.md (551 lines)
      - chapters/6-patterns/7-progressive-disclosure.md (307 lines)

      All follow 10-section structure with Core Structure, Implementation,
      When to Use, Trade-offs, Comparison, Anti-Patterns, Production Considerations,
      Connections. ReAct includes CoT comparison, HITL includes approval gates section,
      Progressive Disclosure includes lazy loading vs eager loading comparison.

    when_to_use:
      - Creating foundational pattern entries for Part 2 Craft
      - Documenting patterns with multiple implementation variants
      - Patterns requiring clear positioning against alternatives
      - Reference material requiring comprehensive coverage

external_research_integration:
  name: Claude-Sneakpeek Research Integration Pattern
  context: Integration of multi-agent orchestration research from community tools into book content
  timestamp: 2026-01-30
  pattern_observed: |
    External research integration reveals sophisticated orchestration patterns that enhance
    foundational content with evidence-grounded learnings. The claude-sneakpeek research
    (Kimi K2.5 swarm mode, TeammateTool feature gates, cc-mirror orchestration skills)
    was successfully integrated across 4 key files without losing existing voice or structure.

    **Integration Strategy:**
    1. **Lightweight Injection Points**: Added timestamped insights to existing sections
       (e.g., Model-Native Swarm section in multi-model-architectures.md)
    2. **Dedicated New Sections**: TeammateTool warranted its own comprehensive section
       (claude-code.md lines 245-524, 280 lines)
    3. **Rich Cross-Reference Anchoring**: Added explicit connections from new content to
       related chapters (orchestrator-pattern.md → multi-model-architectures.md)
    4. **Bidirectional Navigation**: Both orchestrator pattern and multi-agent context updated
       to reference new TeammateTool capabilities

    **Content Quality Maintained:**
    - Voice consistency preserved (third-person, evidence-grounded style)
    - Newcomer cross-references included (TeammateTool to Task comparison table)
    - Mental models explicit (Your Mental Model section for TeammateTool)
    - Trade-offs documented (three-tier spawn strategy, coordination pattern selection)

  key_techniques:
    - Staged integration: Metadata first (brief mention), then full section (comprehensive)
    - Comparative framing: Always position new content against known patterns (Task vs TeammateTool)
    - Source attribution: Include commit hashes and external URLs for traceability
    - Temporal anchoring: Use timestamps to show when external patterns were discovered
    - Forward links: Add "See Also" sections even in non-deterministic exploration

  evidence: |
    **Commit de68e9f (2026-01-30)**: TeammateTool and model-native swarm documentation
    - Added Model-Native Swarm Orchestration section (multi-model-architectures.md)
    - Added TeammateTool section (claude-code.md, 280 lines, 5 coordination patterns)
    - Added Feature Gate Reverse Engineering section (claude-code.md, 190 lines)
    - Cross-reference updates to orchestrator-pattern.md and execution-topologies.md

    **Commit 20500f1 (2026-01-30)**: Knowledge base overhaul expanding research patterns
    - Orchestrator pattern extended: 417 → 674 lines (257 line addition)
    - Added Conductor Philosophy (communication excellence patterns)
    - Added Read vs Delegate Guidelines (1-2 file threshold heuristic)
    - Added Background Execution Mechanics (run_in_background usage)
    - Added Pattern Composition (PR review, feature implementation, bug diagnosis)

    **Files Modified by Research Integration:**
    - chapters/3-model/1-model-selection.md: Multi-Agent Model Selection section
    - chapters/5-tool-use/1-tool-design.md: Rich User Questioning Patterns (4×4 maximal)
    - chapters/6-patterns/3-orchestrator-pattern.md: Conductor philosophy + composition patterns
    - chapters/9-practitioner-toolkit/1-claude-code.md: TeammateTool + feature gates

  when_to_use:
    - Integrating external research (community tools, published papers, practitioner blogs)
    - Filling gaps identified in current documentation
    - Adding evidence for existing patterns with new concrete examples
    - Cross-domain pattern discovery from other specialized communities

  research_to_implementation_workflow:
  - name: Research Collection → Book Content Pathway
    steps:
      1. Scout external sources (community tools, code repositories, research papers)
      2. Extract patterns with evidence and timestamps
      3. Create lightweight timestamped section in related entry
      4. Add full section if pattern warrants dedicated exploration
      5. Create bidirectional cross-references with contextual explanation
      6. Include source attribution and commit references

  communication_pattern_discovery:
    name: Communication Excellence Patterns from Production Orchestrators
    context: Patterns discovered from cc-mirror orchestration skill research
    timestamp: 2026-01-30
    insight: |
      *[2026-01-30]*: Production orchestrators reveal sophisticated communication patterns
      that transform user experience from "watching automation" to "intelligent collaboration."
      The Conductor Philosophy ("absorb complexity, radiate simplicity") describes orchestrators
      that shield users from internal machinery while maintaining transparency about progress.

      Key insight: Forbidden vocabulary (e.g., "spawning subagents" → "parallel tracks")
      creates more natural communication than technical jargon. Vibe-reading adaptation
      (detecting user energy and adjusting pace) indicates mature agent systems anticipate
      user state beyond explicit input.

      This pattern emerged from cc-mirror research but applies broadly to orchestrator
      design—not just Claude Code tooling. The distinction between "scheduler" and
      "conductor" is behavioral, not architectural.

    evidence: |
      - cc-mirror orchestration skill SKILL.md (Conductor philosophy section)
      - Communication Guide with 4 user states and adaptive responses
      - Forbidden vocabulary mapping (8 technical terms → natural alternatives)

  user_questioning_pattern:
    name: 4×4 Rich Question Pattern for Scope Clarification
    context: Pattern from cc-mirror orchestration research
    timestamp: 2026-01-30
    insight: |
      *[2026-01-30]*: User questioning patterns evolved from binary text menus to rich,
      decision-guiding structures. The 4×4 Maximal Pattern (4 questions × 4 options each)
      replaces "Which do you prefer?" with structured decision support.

      Core insight: "Users don't know what they want until they see the options."
      Each option includes description, trade-offs, and recommended guidance.
      Multi-select support enables users to combine attributes rather than binary choices.

      This pattern bridges tool design (AskUserQuestion) and orchestration (scope clarification
      before delegation). Belongs in tool-design.md but originates from orchestration research.

    evidence: |
      - cc-mirror tool-design reference (Rich User Questioning Patterns)
      - Feature implementation scope clarification example (4×4 structure)
      - Anti-pattern: Text-based menus compared to rich structures

  read_vs_delegate_heuristic:
    name: 1-2 File Threshold for Orchestrator Direct Reads
    context: Pattern from production orchestrator analysis
    timestamp: 2026-01-30
    insight: |
      *[2026-01-30]*: Orchestrators read directly (1-2 files) for reference, delegate (3+ files)
      for exploration. Direct reads maintain synthesis capacity; delegation frees context while
      enabling parallelism. Anti-pattern: Reading 15 files directly to understand a module
      pollutes context. Pattern: Spawn agent, receive "OAuth2 with custom middleware in 3 layers"
      summary, maintain clean context for next decision.

  background_execution_pattern:
    name: Default to Background Execution for Parallelism
    context: Pattern from TeammateTool and orchestration research
    timestamp: 2026-01-30
    insight: |
      *[2026-01-30]*: Background execution (run_in_background=True) is fundamental to
      parallelism, not a feature toggle. Production orchestrators treat background as
      the default mental model—foreground execution serializes work.

      Key distinction: Background-first enables orchestrator to spawn multiple agents
      and continue coordination work while agents execute. Completion notifications
      prevent polling overhead. Blocking checks (block=True) only when results immediately
      needed for next decision.

      Mental model shift: Background isn't "run this later"—it's "run this concurrently
      while I coordinate other work." Foreground is the exception for simple, single-agent
      delegation requiring no parallelism.

    evidence: |
      - cc-mirror orchestration skill (Background Execution Mechanics section)
      - TeammateTool documentation (Join vs background completion semantics)
      - Comparison table: background vs foreground trade-offs

  pattern_composition_framework:
    name: Real-World Pattern Composition (Fan-Out + Pipeline + Background)
    context: Production orchestrator patterns from cc-mirror research
    timestamp: 2026-01-30
    insight: |
      *[2026-01-30]*: Real orchestration composes fundamentals into complex workflows.
      Three exemplars: PR Review (Fan-Out + Map-Reduce with 3 parallel reviewers),
      Feature Implementation (Pipeline + Fan-Out + Background escalating Haiku→Opus→Sonnet),
      Bug Diagnosis (Fan-Out + Pipeline with parallel investigation then sequential fix).
      Key: Patterns nest through nesting primitives (SequentialAgent, ParallelAgent, LoopAgent
      from Google ADK). Claude Code achieves similar effects through single-message parallelism.

stability:
  oscillation_detection:
    rule: |
      IF entry E1 at T1 contradicts E0 at T0
      AND entry E2 at T2 contradicts E1
      THEN oscillation detected
    resolution: |
      Preserve both with conflict marker
      Escalate to human
    detected_oscillations: []

  convergence_indicators:
    insight_rate_trend: "Accelerating - external research integration patterns, orchestration learnings, composition frameworks"
    new_entries_this_cycle: 6  # communication patterns, questioning patterns, read/delegate heuristic, background execution, pattern composition, research integration workflow
    contradiction_count: 0
    last_reviewed: 2026-01-30
    notes: |
      Knowledge domain active learning from external research integration (commits de68e9f + 20500f1):

      **Research Integration Learnings:**
      - External pattern integration maintains voice consistency through lightweight timestamped injection
      - TeammateTool documentation reveals coordination primitive layer beneath Task tool
      - Feature gate reverse engineering documents observational research patterns
      - Communication patterns from production orchestrators bridge tool design and agent behavior

      **Orchestration Pattern Discoveries:**
      - Conductor Philosophy transforms user experience through vocabulary choices
      - 4×4 Rich Question Pattern replaces binary menus with structured decision support
      - 1-2 File Threshold heuristic guides when orchestrators should read vs delegate
      - Background execution default enables true parallelism in multi-agent systems
      - Pattern composition shows how real workflows nest multiple fundamentals

      **Size governance:** No new size concerns; expertise stable at target.
      **Contradiction rate:** Zero contradictions between new and existing patterns.
      **Utility ratio:** All 6 new entries directly applied in recent commits (high utility).

      Domain growth areas:
      - External research integration workflow (systematic approach to incorporating discoveries)
      - Communication excellence patterns (evolving beyond task execution to collaboration)
      - Orchestration composition frameworks (nesting patterns for complex workflows)
      - Feature gate analysis (reverse engineering production capabilities)
